#Commands to run when trying to execute a job:

Run a slurm job => sbatch <name_of_script>

Find job ID of slurm job => squeue -u <NET-ID>

Search slurm jobs by job ID => squeue -j <JOB-ID>

See all jobs run on account => sacct

See info on specific job => sacct -j <JOB-ID>

To cancel a job by job ID => scancel -j <JOB-ID>


#Commands to run job arrays (similar jobs with same hardware reqs):

Note: Can only be used in batch jobs, input differs in info not structure

Run slurm array job => sbatch --array=1-5 <name_of_script>


#Commands to run GPU job

Add following hardware req to job script => #SBATCH --gres=gpu:1

#Commands to run when trying to execute a job:

Run a slurm job => sbatch <name_of_script>

Find job ID of slurm job => squeue -u <NET-ID>

Search slurm jobs by job ID => squeue -j <JOB-ID>

See all jobs run on account => sacct

See info on specific job => sacct -j <JOB-ID>

To cancel a job by job ID => scancel -j <JOB-ID>


#Commands to run job arrays (similar jobs with same hardware reqs):

Note: Can only be used in batch jobs, input differs in info not structure


#Commands to use Jupyter on Greene Cluster

Copy the jupyter instruction file from NYU => cp /share/apps/examples/jupyter/run-jupyter.sbatch /home/<net-id>

Run the following jupyter sbatch file => sbatch run-jupyter.sbatch

view instruction file using vim or nano => vim/nano slurm-<job ID>.out

follow instructions to connect to jupyter server and open server on browser

Note: all files created and executed on this jupyter server will execute on the Greene cluster.

#Singularity command
singularity exec --overlay llm-container.ext3:rw /scratch/work/public/singularity/cuda11.6.124-cudnn8.4.0.27-devel-ubuntu20.04.4.sif /bin/bash

srun --cpus-per-task=2 --mem=10GB --time=04:00:00 --pty /bin/bash

source /ext3/env.sh

#vLLM setup:

singularity exec --nv --bind /home/ms9761/tmp/ld.so.cache:/etc/ld.so.cache:ro --overlay /scratch/ms9761/rea-llm/llm-container.ext3:rw /scratch/work/public/singularity/cuda12.2.2-cudnn8.9.4-devel-ubuntu22.04.3.sif source /ext3/env.sh; conda activate llama-env; export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-12.2/compat; python3 -u -m vllm.entrypoints.api_server --host 0.0.0.0 --port 8000 --model mistralai/Mixtral-8x7B-Instruct-v0.1 --dtype float16 --download-dir /scratch/ms9761/rea-llm/data --tensor-parallel-size 4 --load-format safetensors

source /ext3/env.sh

conda activate llama-env

export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-12.2/compat

python3 -u -m vllm.entrypoints.api_server --host 0.0.0.0 --port 8000 --model mistralai/Mixtral-8x7B-Instruct-v0.1 --dtype float16 --download-dir /scratch/ms9761/rea-llm/data --tensor-parallel-size 4 --load-format safetensors

