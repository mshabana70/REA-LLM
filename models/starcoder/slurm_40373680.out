Server is not up yet; status=000
[2m2023-11-25T19:10:41.654380Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Args { model_id: "GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct", revision: None, validation_workers: 2, sharded: None, num_shard: None, quantize: None, dtype: None, trust_remote_code: false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences: 4, max_top_n_tokens: 5, max_input_length: 4000, max_total_tokens: 6000, waiting_served_ratio: 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens: None, max_waiting_tokens: 20, hostname: "0.0.0.0", port: 8000, shard_uds_path: "/tmp/text-generation-server", master_addr: "localhost", master_port: 29500, huggingface_hub_cache: Some("/scratch/ms9761/rea-llm/data"), weights_cache_override: None, disable_custom_kernels: true, cuda_memory_fraction: 1.0, rope_scaling: None, rope_factor: None, json_output: false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken: None, ngrok_edge: None, env: false }
[2m2023-11-25T19:10:41.654509Z[0m [32m INFO[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Starting download process.
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
[2m2023-11-25T19:10:49.943348Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Files are already present on the host. Skipping download.

Server is not up yet; status=000
[2m2023-11-25T19:10:50.898459Z[0m [32m INFO[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Successfully downloaded weights.
[2m2023-11-25T19:10:50.898707Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Starting shard [2m[3mrank[0m[2m=[0m0[0m
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
[2m2023-11-25T19:10:54.016448Z[0m [33m WARN[0m [2mtext_generation_launcher[0m[2m:[0m We're not using custom kernels.

[2m2023-11-25T19:10:54.166992Z[0m [33m WARN[0m [2mtext_generation_launcher[0m[2m:[0m Unable to use Flash Attention V2: GPU with CUDA capability 7 5 is not supported for Flash Attention V2

[2m2023-11-25T19:10:54.197330Z[0m [33m WARN[0m [2mtext_generation_launcher[0m[2m:[0m Could not import Mistral model: Mistral model requires flash attn v2

Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
[2m2023-11-25T19:11:00.937331Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Waiting for shard to be ready... [2m[3mrank[0m[2m=[0m0[0m
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
[2m2023-11-25T19:11:10.955327Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Waiting for shard to be ready... [2m[3mrank[0m[2m=[0m0[0m
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
[2m2023-11-25T19:11:20.975326Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Waiting for shard to be ready... [2m[3mrank[0m[2m=[0m0[0m
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
[2m2023-11-25T19:11:30.987332Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Waiting for shard to be ready... [2m[3mrank[0m[2m=[0m0[0m
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
[2m2023-11-25T19:11:41.007368Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Waiting for shard to be ready... [2m[3mrank[0m[2m=[0m0[0m
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
[2m2023-11-25T19:11:49.744836Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Server started at unix:///tmp/text-generation-server-0

[2m2023-11-25T19:11:49.819407Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Shard ready in 58.911498629s [2m[3mrank[0m[2m=[0m0[0m
[2m2023-11-25T19:11:49.902463Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Starting Webserver
Server is not up yet; status=000
[2m2023-11-25T19:11:50.148243Z[0m [33m WARN[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m166:[0m Could not find a fast tokenizer implementation for GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct
[2m2023-11-25T19:11:50.148267Z[0m [33m WARN[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m169:[0m Rust input length validation and truncation is disabled
[2m2023-11-25T19:11:50.148271Z[0m [33m WARN[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m349:[0m `--revision` is not set
[2m2023-11-25T19:11:50.148274Z[0m [33m WARN[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m350:[0m We strongly advise to set it to a known supported commit.
[2m2023-11-25T19:11:50.229166Z[0m [32m INFO[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m371:[0m Serving revision ba5272b6be9b0a03706b2e85a1307d8ba239e93c of model GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct
[2m2023-11-25T19:11:50.235298Z[0m [32m INFO[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m213:[0m Warming up model
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
Server is not up yet; status=000
[2m2023-11-25T19:11:55.797981Z[0m [32m INFO[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m246:[0m Setting max batch total tokens to 687600
[2m2023-11-25T19:11:55.797996Z[0m [32m INFO[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m247:[0m Connected
Server is up!
[2m2023-11-25T19:12:00.282326Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(2000), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"2.321211054s" [3mvalidation_time[0m[2m=[0m"355.263Âµs" [3mqueue_time[0m[2m=[0m"45.019Âµs" [3minference_time[0m[2m=[0m"2.320810947s" [3mtime_per_token[0m[2m=[0m"145.050684ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m289:[0m Success
[2m2023-11-25T19:12:04.012183Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(2000), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"3.726890851s" [3mvalidation_time[0m[2m=[0m"330.103Âµs" [3mqueue_time[0m[2m=[0m"32.216Âµs" [3minference_time[0m[2m=[0m"3.726528729s" [3mtime_per_token[0m[2m=[0m"103.514686ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m289:[0m Success
[2m2023-11-25T19:12:05.481006Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(2000), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"1.466098023s" [3mvalidation_time[0m[2m=[0m"104.185Âµs" [3mqueue_time[0m[2m=[0m"29.973Âµs" [3minference_time[0m[2m=[0m"1.465963988s" [3mtime_per_token[0m[2m=[0m"91.622749ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m289:[0m Success
[2m2023-11-25T19:12:07.488667Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(2000), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"2.005007785s" [3mvalidation_time[0m[2m=[0m"102.145Âµs" [3mqueue_time[0m[2m=[0m"27.825Âµs" [3minference_time[0m[2m=[0m"2.004877965s" [3mtime_per_token[0m[2m=[0m"83.536581ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m289:[0m Success
[2m2023-11-25T19:12:09.685578Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(2000), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"2.194174638s" [3mvalidation_time[0m[2m=[0m"105.33Âµs" [3mqueue_time[0m[2m=[0m"36.63Âµs" [3minference_time[0m[2m=[0m"2.194032855s" [3mtime_per_token[0m[2m=[0m"73.134427ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m289:[0m Success
[2m2023-11-25T19:12:12.344403Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(2000), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"2.656245829s" [3mvalidation_time[0m[2m=[0m"100.93Âµs" [3mqueue_time[0m[2m=[0m"27.344Âµs" [3minference_time[0m[2m=[0m"2.656117726s" [3mtime_per_token[0m[2m=[0m"71.786965ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m289:[0m Success
[2m2023-11-25T19:12:14.544622Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(2000), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"2.179040587s" [3mvalidation_time[0m[2m=[0m"55.91Âµs" [3mqueue_time[0m[2m=[0m"36.154Âµs" [3minference_time[0m[2m=[0m"2.178948712s" [3mtime_per_token[0m[2m=[0m"66.028748ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m289:[0m Success
[2m2023-11-25T19:12:17.503555Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(2000), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"2.956212465s" [3mvalidation_time[0m[2m=[0m"54.166Âµs" [3mqueue_time[0m[2m=[0m"28.528Âµs" [3minference_time[0m[2m=[0m"2.956129913s" [3mtime_per_token[0m[2m=[0m"65.691775ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m289:[0m Success
[2m2023-11-25T19:12:19.000792Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(2000), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"1.494635241s" [3mvalidation_time[0m[2m=[0m"182.873Âµs" [3mqueue_time[0m[2m=[0m"31.225Âµs" [3minference_time[0m[2m=[0m"1.494421303s" [3mtime_per_token[0m[2m=[0m"93.401331ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m289:[0m Success
[2m2023-11-25T19:12:21.373556Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(2000), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"2.370268425s" [3mvalidation_time[0m[2m=[0m"187.171Âµs" [3mqueue_time[0m[2m=[0m"30.253Âµs" [3minference_time[0m[2m=[0m"2.370051165s" [3mtime_per_token[0m[2m=[0m"81.725902ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m289:[0m Success
running python request script
